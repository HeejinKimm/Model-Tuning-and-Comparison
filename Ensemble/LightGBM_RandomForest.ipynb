{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 1) 라이브러리 불러오기\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score"
      ],
      "metadata": {
        "id": "l5QO7JsFPdmA"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Load data form drive\n",
        "data = pd.read_csv('/content/gdrive/MyDrive/2025-1 Pattern recognition/train_processed.csv', engine='python')\n",
        "data.info()\n",
        "\n",
        "\n",
        "# 피처/타깃 분리\n",
        "target_col = 'y'\n",
        "feature_cols = [c for c in data.columns if c != target_col]\n",
        "\n",
        "X = data[feature_cols]\n",
        "y = data[target_col]\n",
        "\n",
        "# 피처셋 구성: id, shares, 원본 y 모두 제거\n",
        "X = data.drop(columns=['id', 'shares', 'y'])\n",
        "\n",
        "# 칼럼명 공백 → 언더스코어\n",
        "X.columns = [c.strip().replace(' ', '_') for c in X.columns]\n",
        "\n",
        "def drop_corr_features(X, threshold):\n",
        "    # 1) 원본 상관행렬 계산\n",
        "    corr = X.corr()\n",
        "    # 2) 상삼각(주대각선 위)만 남기기\n",
        "    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
        "    # 3) 양(+) 또는 음(–) 상관관계가 threshold보다 클 경우 제거 대상\n",
        "    to_drop = [\n",
        "        col for col in upper.columns\n",
        "        if (upper[col] >  threshold).any()  # 강한 양의 상관\n",
        "        or (upper[col] < -threshold).any()  # 강한 음의 상관\n",
        "    ]\n",
        "    return to_drop\n",
        "\n",
        "# 사용 예시\n",
        "to_drop = drop_corr_features(X, threshold=0.8)\n",
        "print(f\"Dropping {len(to_drop)} high-corr features (>|0.8|):\", to_drop)\n",
        "\n",
        "# 제거 후 데이터\n",
        "X_reduced = X.drop(columns=to_drop)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZn3KZMLPf_y",
        "outputId": "cf69a104-2311-40ad-e5d9-18c303b1bffc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 22200 entries, 0 to 22199\n",
            "Data columns (total 60 columns):\n",
            " #   Column                        Non-Null Count  Dtype  \n",
            "---  ------                        --------------  -----  \n",
            " 0   id                            22200 non-null  int64  \n",
            " 1   n_tokens_title                22200 non-null  float64\n",
            " 2   n_tokens_content              22200 non-null  float64\n",
            " 3   n_unique_tokens               22200 non-null  float64\n",
            " 4   n_non_stop_words              22200 non-null  float64\n",
            " 5   n_non_stop_unique_tokens      22200 non-null  float64\n",
            " 6   num_hrefs                     22200 non-null  float64\n",
            " 7   num_self_hrefs                22200 non-null  float64\n",
            " 8   num_imgs                      22200 non-null  float64\n",
            " 9   num_videos                    22200 non-null  float64\n",
            " 10  average_token_length          22200 non-null  float64\n",
            " 11  num_keywords                  22200 non-null  float64\n",
            " 12  kw_min_min                    22200 non-null  float64\n",
            " 13  kw_max_min                    22200 non-null  float64\n",
            " 14  kw_avg_min                    22200 non-null  float64\n",
            " 15  kw_min_max                    22200 non-null  float64\n",
            " 16  kw_max_max                    22200 non-null  float64\n",
            " 17  kw_avg_max                    22200 non-null  float64\n",
            " 18  kw_min_avg                    22200 non-null  float64\n",
            " 19  kw_max_avg                    22200 non-null  float64\n",
            " 20  kw_avg_avg                    22200 non-null  float64\n",
            " 21  self_reference_min_shares     22200 non-null  float64\n",
            " 22  self_reference_max_shares     22200 non-null  float64\n",
            " 23  self_reference_avg_sharess    22200 non-null  float64\n",
            " 24  LDA_00                        22200 non-null  float64\n",
            " 25  LDA_01                        22200 non-null  float64\n",
            " 26  LDA_02                        22200 non-null  float64\n",
            " 27  LDA_03                        22200 non-null  float64\n",
            " 28  LDA_04                        22200 non-null  float64\n",
            " 29  global_subjectivity           22200 non-null  float64\n",
            " 30  global_sentiment_polarity     22200 non-null  float64\n",
            " 31  global_rate_positive_words    22200 non-null  float64\n",
            " 32  global_rate_negative_words    22200 non-null  float64\n",
            " 33  rate_positive_words           22200 non-null  float64\n",
            " 34  rate_negative_words           22200 non-null  float64\n",
            " 35  avg_positive_polarity         22200 non-null  float64\n",
            " 36  min_positive_polarity         22200 non-null  float64\n",
            " 37  max_positive_polarity         22200 non-null  float64\n",
            " 38  avg_negative_polarity         22200 non-null  float64\n",
            " 39  min_negative_polarity         22200 non-null  float64\n",
            " 40  max_negative_polarity         22200 non-null  float64\n",
            " 41  title_subjectivity            22200 non-null  float64\n",
            " 42  title_sentiment_polarity      22200 non-null  float64\n",
            " 43  abs_title_subjectivity        22200 non-null  float64\n",
            " 44  abs_title_sentiment_polarity  22200 non-null  float64\n",
            " 45  channel_Business              22200 non-null  bool   \n",
            " 46  channel_Entertainment         22200 non-null  bool   \n",
            " 47  channel_Lifestyle             22200 non-null  bool   \n",
            " 48  channel_Social Media          22200 non-null  bool   \n",
            " 49  channel_Tech                  22200 non-null  bool   \n",
            " 50  channel_World                 22200 non-null  bool   \n",
            " 51  weekday_Monday                22200 non-null  bool   \n",
            " 52  weekday_Tuesday               22200 non-null  bool   \n",
            " 53  weekday_Wednesday             22200 non-null  bool   \n",
            " 54  weekday_Thursday              22200 non-null  bool   \n",
            " 55  weekday_Friday                22200 non-null  bool   \n",
            " 56  weekday_Saturday              22200 non-null  bool   \n",
            " 57  weekday_Sunday                22200 non-null  bool   \n",
            " 58  shares                        22200 non-null  int64  \n",
            " 59  y                             22200 non-null  int64  \n",
            "dtypes: bool(13), float64(44), int64(3)\n",
            "memory usage: 8.2 MB\n",
            "Dropping 5 high-corr features (>|0.8|): ['n_non_stop_unique_tokens', 'average_token_length', 'kw_min_avg', 'self_reference_max_shares', 'self_reference_avg_sharess']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) 데이터 준비 (예시: 유방암 진단 데이터)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# 3) 기본 모델 정의\n",
        "lgbm = LGBMClassifier(\n",
        "    n_estimators=100,       # 트리 개수\n",
        "    learning_rate=0.05,     # 학습률\n",
        "    num_leaves=31,          # 하나의 트리가 가질 수 있는 잎사귀 최대 개수\n",
        "    max_depth=-1,           # 트리 최대 깊이\n",
        "    min_child_samples=20,   # 리프 하나가 갖춰야 할 최소 데이터 수\n",
        "    subsample=1.0,          # row 샘플링 비율 (bagging)\n",
        "    colsample_bytree=1.0,   # feature 샘플링 비율\n",
        "    reg_alpha=0.0,          # L1 규제\n",
        "    reg_lambda=0.0,         # L2 규제\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=100,       # 나무 개수\n",
        "    max_depth=None,         # 트리 최대 깊이\n",
        "    max_features='sqrt',    # 분할 시 고려할 feature 비율\n",
        "    min_samples_split=2,    # 내부 노드를 분할하기 위한 최소 샘플 수\n",
        "    min_samples_leaf=1,     # 리프가 되기 위한 최소 샘플 수\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "lr   = LogisticRegression(max_iter=1000, random_state=42)"
      ],
      "metadata": {
        "id": "mDYbybRuRtej"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 방법 A: Voting Ensemble ---\n",
        "voting_hard = VotingClassifier(\n",
        "    estimators=[('lgbm', lgbm), ('rf', rf)],\n",
        "    voting='hard'               # 'soft'로 바꾸면 클래스별 확률 평균\n",
        ")\n",
        "\n",
        "voting_soft = VotingClassifier(\n",
        "    estimators=[('lgbm', lgbm), ('rf', rf)],\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "# 교차검증으로 성능 확인\n",
        "for name, model in [('Voting-hard', voting_hard), ('Voting-soft', voting_soft)]:\n",
        "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
        "    print(f\"{name} CV Accuracy: {scores.mean():.4f} ± {scores.std():.4f}\")\n",
        "\n",
        "# 모델 학습 및 평가\n",
        "voting_soft.fit(X_train, y_train)\n",
        "print(\"Voting-soft Test Acc:\", voting_soft.score(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSRYhuwnRxEG",
        "outputId": "a38b0274-842b-4570-9324-aeac4638cc7f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 7043, number of negative: 7165\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005271 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8578\n",
            "[LightGBM] [Info] Number of data points in the train set: 14208, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495707 -> initscore=-0.017174\n",
            "[LightGBM] [Info] Start training from score -0.017174\n",
            "[LightGBM] [Info] Number of positive: 7043, number of negative: 7165\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002710 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8583\n",
            "[LightGBM] [Info] Number of data points in the train set: 14208, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495707 -> initscore=-0.017174\n",
            "[LightGBM] [Info] Start training from score -0.017174\n",
            "[LightGBM] [Info] Number of positive: 7042, number of negative: 7166\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004301 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8576\n",
            "[LightGBM] [Info] Number of data points in the train set: 14208, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495636 -> initscore=-0.017455\n",
            "[LightGBM] [Info] Start training from score -0.017455\n",
            "[LightGBM] [Info] Number of positive: 7042, number of negative: 7166\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002674 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8578\n",
            "[LightGBM] [Info] Number of data points in the train set: 14208, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495636 -> initscore=-0.017455\n",
            "[LightGBM] [Info] Start training from score -0.017455\n",
            "[LightGBM] [Info] Number of positive: 7042, number of negative: 7166\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002633 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8573\n",
            "[LightGBM] [Info] Number of data points in the train set: 14208, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495636 -> initscore=-0.017455\n",
            "[LightGBM] [Info] Start training from score -0.017455\n",
            "Voting-hard CV Accuracy: 0.6486 ± 0.0057\n",
            "[LightGBM] [Info] Number of positive: 7043, number of negative: 7165\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002558 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8578\n",
            "[LightGBM] [Info] Number of data points in the train set: 14208, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495707 -> initscore=-0.017174\n",
            "[LightGBM] [Info] Start training from score -0.017174\n",
            "[LightGBM] [Info] Number of positive: 7043, number of negative: 7165\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004248 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8583\n",
            "[LightGBM] [Info] Number of data points in the train set: 14208, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495707 -> initscore=-0.017174\n",
            "[LightGBM] [Info] Start training from score -0.017174\n",
            "[LightGBM] [Info] Number of positive: 7042, number of negative: 7166\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002952 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8576\n",
            "[LightGBM] [Info] Number of data points in the train set: 14208, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495636 -> initscore=-0.017455\n",
            "[LightGBM] [Info] Start training from score -0.017455\n",
            "[LightGBM] [Info] Number of positive: 7042, number of negative: 7166\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003270 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8578\n",
            "[LightGBM] [Info] Number of data points in the train set: 14208, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495636 -> initscore=-0.017455\n",
            "[LightGBM] [Info] Start training from score -0.017455\n",
            "[LightGBM] [Info] Number of positive: 7042, number of negative: 7166\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002678 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8573\n",
            "[LightGBM] [Info] Number of data points in the train set: 14208, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495636 -> initscore=-0.017455\n",
            "[LightGBM] [Info] Start training from score -0.017455\n",
            "Voting-soft CV Accuracy: 0.6568 ± 0.0067\n",
            "[LightGBM] [Info] Number of positive: 8803, number of negative: 8957\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005352 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8595\n",
            "[LightGBM] [Info] Number of data points in the train set: 17760, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495664 -> initscore=-0.017343\n",
            "[LightGBM] [Info] Start training from score -0.017343\n",
            "Voting-soft Test Acc: 0.6686936936936937\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2rpaQt8OoZn",
        "outputId": "095f7316-0786-411e-e33d-f5f8e8ba3a03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 7043, number of negative: 7165\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003132 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8578\n",
            "[LightGBM] [Info] Number of data points in the train set: 14208, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495707 -> initscore=-0.017174\n",
            "[LightGBM] [Info] Start training from score -0.017174\n",
            "[LightGBM] [Info] Number of positive: 5634, number of negative: 5732\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002138 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8549\n",
            "[LightGBM] [Info] Number of data points in the train set: 11366, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495689 -> initscore=-0.017245\n",
            "[LightGBM] [Info] Start training from score -0.017245\n",
            "[LightGBM] [Info] Number of positive: 5634, number of negative: 5732\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002207 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8535\n",
            "[LightGBM] [Info] Number of data points in the train set: 11366, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495689 -> initscore=-0.017245\n",
            "[LightGBM] [Info] Start training from score -0.017245\n",
            "[LightGBM] [Info] Number of positive: 5634, number of negative: 5732\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002142 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8558\n",
            "[LightGBM] [Info] Number of data points in the train set: 11366, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495689 -> initscore=-0.017245\n",
            "[LightGBM] [Info] Start training from score -0.017245\n",
            "[LightGBM] [Info] Number of positive: 5635, number of negative: 5732\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002648 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8546\n",
            "[LightGBM] [Info] Number of data points in the train set: 11367, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495733 -> initscore=-0.017067\n",
            "[LightGBM] [Info] Start training from score -0.017067\n",
            "[LightGBM] [Info] Number of positive: 5635, number of negative: 5732\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002200 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8540\n",
            "[LightGBM] [Info] Number of data points in the train set: 11367, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495733 -> initscore=-0.017067\n",
            "[LightGBM] [Info] Start training from score -0.017067\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 7043, number of negative: 7165\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002654 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8583\n",
            "[LightGBM] [Info] Number of data points in the train set: 14208, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495707 -> initscore=-0.017174\n",
            "[LightGBM] [Info] Start training from score -0.017174\n",
            "[LightGBM] [Info] Number of positive: 5634, number of negative: 5732\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007919 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 8567\n",
            "[LightGBM] [Info] Number of data points in the train set: 11366, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495689 -> initscore=-0.017245\n",
            "[LightGBM] [Info] Start training from score -0.017245\n",
            "[LightGBM] [Info] Number of positive: 5634, number of negative: 5732\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002248 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8553\n",
            "[LightGBM] [Info] Number of data points in the train set: 11366, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495689 -> initscore=-0.017245\n",
            "[LightGBM] [Info] Start training from score -0.017245\n",
            "[LightGBM] [Info] Number of positive: 5634, number of negative: 5732\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002157 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8562\n",
            "[LightGBM] [Info] Number of data points in the train set: 11366, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495689 -> initscore=-0.017245\n",
            "[LightGBM] [Info] Start training from score -0.017245\n",
            "[LightGBM] [Info] Number of positive: 5635, number of negative: 5732\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002600 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8557\n",
            "[LightGBM] [Info] Number of data points in the train set: 11367, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495733 -> initscore=-0.017067\n",
            "[LightGBM] [Info] Start training from score -0.017067\n",
            "[LightGBM] [Info] Number of positive: 5635, number of negative: 5732\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002218 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8557\n",
            "[LightGBM] [Info] Number of data points in the train set: 11367, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495733 -> initscore=-0.017067\n",
            "[LightGBM] [Info] Start training from score -0.017067\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 7042, number of negative: 7166\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002729 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8576\n",
            "[LightGBM] [Info] Number of data points in the train set: 14208, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495636 -> initscore=-0.017455\n",
            "[LightGBM] [Info] Start training from score -0.017455\n",
            "[LightGBM] [Info] Number of positive: 5634, number of negative: 5732\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002431 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8546\n",
            "[LightGBM] [Info] Number of data points in the train set: 11366, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495689 -> initscore=-0.017245\n",
            "[LightGBM] [Info] Start training from score -0.017245\n",
            "[LightGBM] [Info] Number of positive: 5633, number of negative: 5733\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002243 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8544\n",
            "[LightGBM] [Info] Number of data points in the train set: 11366, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495601 -> initscore=-0.017597\n",
            "[LightGBM] [Info] Start training from score -0.017597\n",
            "[LightGBM] [Info] Number of positive: 5633, number of negative: 5733\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002195 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8554\n",
            "[LightGBM] [Info] Number of data points in the train set: 11366, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495601 -> initscore=-0.017597\n",
            "[LightGBM] [Info] Start training from score -0.017597\n",
            "[LightGBM] [Info] Number of positive: 5634, number of negative: 5733\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002154 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8540\n",
            "[LightGBM] [Info] Number of data points in the train set: 11367, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495645 -> initscore=-0.017419\n",
            "[LightGBM] [Info] Start training from score -0.017419\n",
            "[LightGBM] [Info] Number of positive: 5634, number of negative: 5733\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007584 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 8545\n",
            "[LightGBM] [Info] Number of data points in the train set: 11367, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495645 -> initscore=-0.017419\n",
            "[LightGBM] [Info] Start training from score -0.017419\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 7042, number of negative: 7166\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002626 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8578\n",
            "[LightGBM] [Info] Number of data points in the train set: 14208, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495636 -> initscore=-0.017455\n",
            "[LightGBM] [Info] Start training from score -0.017455\n",
            "[LightGBM] [Info] Number of positive: 5634, number of negative: 5732\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002587 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8549\n",
            "[LightGBM] [Info] Number of data points in the train set: 11366, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495689 -> initscore=-0.017245\n",
            "[LightGBM] [Info] Start training from score -0.017245\n",
            "[LightGBM] [Info] Number of positive: 5633, number of negative: 5733\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002411 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8546\n",
            "[LightGBM] [Info] Number of data points in the train set: 11366, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495601 -> initscore=-0.017597\n",
            "[LightGBM] [Info] Start training from score -0.017597\n",
            "[LightGBM] [Info] Number of positive: 5633, number of negative: 5733\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002424 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8566\n",
            "[LightGBM] [Info] Number of data points in the train set: 11366, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495601 -> initscore=-0.017597\n",
            "[LightGBM] [Info] Start training from score -0.017597\n",
            "[LightGBM] [Info] Number of positive: 5634, number of negative: 5733\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007351 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 8543\n",
            "[LightGBM] [Info] Number of data points in the train set: 11367, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495645 -> initscore=-0.017419\n",
            "[LightGBM] [Info] Start training from score -0.017419\n",
            "[LightGBM] [Info] Number of positive: 5634, number of negative: 5733\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008513 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 8555\n",
            "[LightGBM] [Info] Number of data points in the train set: 11367, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495645 -> initscore=-0.017419\n",
            "[LightGBM] [Info] Start training from score -0.017419\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 7042, number of negative: 7166\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009181 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 8573\n",
            "[LightGBM] [Info] Number of data points in the train set: 14208, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495636 -> initscore=-0.017455\n",
            "[LightGBM] [Info] Start training from score -0.017455\n",
            "[LightGBM] [Info] Number of positive: 5634, number of negative: 5732\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002123 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8550\n",
            "[LightGBM] [Info] Number of data points in the train set: 11366, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495689 -> initscore=-0.017245\n",
            "[LightGBM] [Info] Start training from score -0.017245\n",
            "[LightGBM] [Info] Number of positive: 5633, number of negative: 5733\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002152 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8536\n",
            "[LightGBM] [Info] Number of data points in the train set: 11366, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495601 -> initscore=-0.017597\n",
            "[LightGBM] [Info] Start training from score -0.017597\n",
            "[LightGBM] [Info] Number of positive: 5633, number of negative: 5733\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002159 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8536\n",
            "[LightGBM] [Info] Number of data points in the train set: 11366, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495601 -> initscore=-0.017597\n",
            "[LightGBM] [Info] Start training from score -0.017597\n",
            "[LightGBM] [Info] Number of positive: 5634, number of negative: 5733\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003629 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8554\n",
            "[LightGBM] [Info] Number of data points in the train set: 11367, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495645 -> initscore=-0.017419\n",
            "[LightGBM] [Info] Start training from score -0.017419\n",
            "[LightGBM] [Info] Number of positive: 5634, number of negative: 5733\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002218 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8552\n",
            "[LightGBM] [Info] Number of data points in the train set: 11367, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495645 -> initscore=-0.017419\n",
            "[LightGBM] [Info] Start training from score -0.017419\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking CV Accuracy: 0.6093 ± 0.0057\n",
            "[LightGBM] [Info] Number of positive: 8803, number of negative: 8957\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013265 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 8595\n",
            "[LightGBM] [Info] Number of data points in the train set: 17760, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495664 -> initscore=-0.017343\n",
            "[LightGBM] [Info] Start training from score -0.017343\n",
            "[LightGBM] [Info] Number of positive: 7043, number of negative: 7165\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002863 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8578\n",
            "[LightGBM] [Info] Number of data points in the train set: 14208, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495707 -> initscore=-0.017174\n",
            "[LightGBM] [Info] Start training from score -0.017174\n",
            "[LightGBM] [Info] Number of positive: 7043, number of negative: 7165\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004359 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8583\n",
            "[LightGBM] [Info] Number of data points in the train set: 14208, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495707 -> initscore=-0.017174\n",
            "[LightGBM] [Info] Start training from score -0.017174\n",
            "[LightGBM] [Info] Number of positive: 7042, number of negative: 7166\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004198 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8576\n",
            "[LightGBM] [Info] Number of data points in the train set: 14208, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495636 -> initscore=-0.017455\n",
            "[LightGBM] [Info] Start training from score -0.017455\n",
            "[LightGBM] [Info] Number of positive: 7042, number of negative: 7166\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014900 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 8578\n",
            "[LightGBM] [Info] Number of data points in the train set: 14208, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495636 -> initscore=-0.017455\n",
            "[LightGBM] [Info] Start training from score -0.017455\n",
            "[LightGBM] [Info] Number of positive: 7042, number of negative: 7166\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004456 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8573\n",
            "[LightGBM] [Info] Number of data points in the train set: 14208, number of used features: 57\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495636 -> initscore=-0.017455\n",
            "[LightGBM] [Info] Start training from score -0.017455\n"
          ]
        }
      ],
      "source": [
        "# --- 방법 B: Stacking Ensemble ---\n",
        "stack = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('lgbm', lgbm),\n",
        "        ('rf', rf),\n",
        "    ],\n",
        "    final_estimator=lr,      # 1차 모델 예측을 입력받아 학습할 메타 모델\n",
        "    cv=5,                    # 스태킹 시 내부 교차검증 설정\n",
        "    passthrough=True         # True면 원본 특성도 메타 모델에 같이 투입\n",
        ")\n",
        "\n",
        "# 스태킹 교차검증\n",
        "stack_scores = cross_val_score(stack, X_train, y_train, cv=5, scoring='accuracy')\n",
        "print(f\"Stacking CV Accuracy: {stack_scores.mean():.4f} ± {stack_scores.std():.4f}\")\n",
        "\n",
        "# 최종 학습 및 테스트\n",
        "stack.fit(X_train, y_train)\n",
        "print(\"Stacking Test Acc:\", stack.score(X_test, y_test))\n"
      ]
    }
  ]
}